{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    static_image_mode=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Indices for key landmarks on the iris and eye corners\n",
    "left_iris_index = 468\n",
    "left_eye_inner = 133\n",
    "left_eye_outer = 33\n",
    "left_eye_top = 159\n",
    "left_eye_bottom = 145\n",
    "\n",
    "right_iris_index = 473\n",
    "right_eye_inner = 362\n",
    "right_eye_outer = 263\n",
    "right_eye_top = 386\n",
    "right_eye_bottom = 374\n",
    "\n",
    "\n",
    "# Function to display labeled landmarks\n",
    "def display_landmarks(frame, face_landmarks, frame_width, frame_height):\n",
    "\n",
    "    # Define key points with names\n",
    "    key_points = (\n",
    "        right_iris_index, # Right Iris Center\n",
    "        left_iris_index,  # Left Iris Center\n",
    "        right_eye_inner,  # Right Eye Inner\n",
    "        right_eye_outer,  # Right Eye Outer\n",
    "        left_eye_inner,   # Left Eye Inner\n",
    "        left_eye_outer,   # Left Eye Outer\n",
    "    )\n",
    "    \n",
    "    # Draw and label each key point\n",
    "    for index in key_points:\n",
    "        x = int(face_landmarks.landmark[index].x * frame_width)\n",
    "        y = int(face_landmarks.landmark[index].y * frame_height)\n",
    "        cv2.circle(frame, (x, y), 3, (0, 255, 255), -1)\n",
    "\n",
    "    \n",
    "# Function to calculate gaze direction\n",
    "def is_concentrated(frame, face_landmarks, frame_width, frame_height):\n",
    "    gaze_text = \"None\"\n",
    "    \n",
    "    # Get positions of left and right iris centers\n",
    "   \n",
    "    left_rye_z = face_landmarks.landmark[left_iris_index].z\n",
    "    right_iris_z = face_landmarks.landmark[right_iris_index].z\n",
    "    \n",
    "    left_eye_y = face_landmarks.landmark[left_eye_outer].y * frame_height\n",
    "    right_eye_y = face_landmarks.landmark[right_eye_outer].y * frame_height\n",
    "    eye_y_difference = abs(left_eye_y - right_eye_y)\n",
    "    \n",
    "    # If the difference in vertical eye positions is too large (indicating a tilted head), return 'Tilted'\n",
    "    if eye_y_difference > 15:\n",
    "        gaze_text = \"Tilted\"\n",
    "        return False, gaze_text\n",
    "    \n",
    "    left_iris_x = face_landmarks.landmark[left_iris_index].x * frame_width\n",
    "    right_iris_x = face_landmarks.landmark[right_iris_index].x * frame_width\n",
    "\n",
    "    # Get eye corner positions\n",
    "    left_eye_inner_x = face_landmarks.landmark[left_eye_inner].x * frame_width\n",
    "    left_eye_outer_x = face_landmarks.landmark[left_eye_outer].x * frame_width\n",
    "    right_eye_inner_x = face_landmarks.landmark[right_eye_inner].x * frame_width\n",
    "    right_eye_outer_x = face_landmarks.landmark[right_eye_outer].x * frame_width\n",
    "\n",
    "    # Calculate relative positions for gaze detection\n",
    "    left_gaze_ratio = (left_iris_x - left_eye_inner_x) / (left_eye_outer_x - left_eye_inner_x)\n",
    "    right_gaze_ratio = (right_iris_x - right_eye_inner_x) / (right_eye_outer_x - right_eye_inner_x)\n",
    "    \n",
    "\n",
    "    cv2.putText(frame, f\"LR: {left_gaze_ratio:.2f}\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 100, 0), 2)\n",
    "    cv2.putText(frame, f\"RR: {right_gaze_ratio:.2f}\", (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 100, 0), 2)\n",
    "    \n",
    "    # Calculate the absolute difference in the Z-coordinate positions of both irises\n",
    "    z_diff = abs(left_rye_z - right_iris_z)\n",
    "    \n",
    "    cv2.putText(frame, f\"ZR: {z_diff:.2f}\", (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 100, 0), 2)\n",
    "    \n",
    "    # Check the head pose by looking at the difference in the Z-coordinate values of the irises\n",
    "    if z_diff > 0.028: # head pose (higher = more angle)\n",
    "        \n",
    "        if (left_rye_z > right_iris_z):  # If the left iris is further away, the user is looking left\n",
    "            gaze_text = \"Looking left\"\n",
    "            if  left_gaze_ratio > 0.55: # higher = more angle\n",
    "                 return False, gaze_text\n",
    "            else:\n",
    "              return True, gaze_text\n",
    "            \n",
    "        else: # looking right\n",
    "            gaze_text = \"Looking right\"\n",
    "            if  right_gaze_ratio > 0.55: # higher = more angle\n",
    "                return False, gaze_text\n",
    "            else:\n",
    "              return True, gaze_text\n",
    "            \n",
    "       \n",
    "    # If the difference between the gaze ratios is small\n",
    "    if abs(left_gaze_ratio - right_gaze_ratio) < 0.14: # higher = more angle\n",
    "        gaze_text = \"Looking Straight\"\n",
    "        return True, gaze_text\n",
    "\n",
    "    return False, gaze_text\n",
    "\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Mirror effect for better visualization\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    # If landmarks are detected, calculate gaze and head position\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Calculate gaze direction and head tilt\n",
    "            concentrated, gaze_text = is_concentrated(frame, face_landmarks, frame_width, frame_height)\n",
    "            \n",
    "            if concentrated:\n",
    "                text = \"Concentrated\"\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                text = \"Not Concentrated\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "            # Display the gaze direction and head tilt on the frame\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "            \n",
    "            cv2.putText(frame, gaze_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "            \n",
    "            \n",
    "            display_landmarks(frame, face_landmarks, frame_width, frame_height)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Eyeball & Head Pose Detection', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "face_mesh.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
